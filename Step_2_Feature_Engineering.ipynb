{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1 Package & Helper Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Mapping Fuction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_yz_to_01(df):\n",
    "    for col in ['authorized_flag', 'category_1']:\n",
    "        df[col] = df[col].map({'Y':1, 'N':0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Fuction (tabular data -> single table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging_transactions(tran_data,df_train,df_test,prefix):  \n",
    "    \n",
    "    tran_data['purchase_date'] = pd.DatetimeIndex(tran_data['purchase_date']).astype(np.int64) * 1e-9\n",
    "\n",
    "    # Step_1 Aggregating\n",
    "    agg_dict = {\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        'category_2_1.0': ['mean','sum'],\n",
    "        'category_2_2.0': ['mean','sum'],\n",
    "        'category_2_3.0': ['mean','sum'],\n",
    "        'category_2_4.0': ['mean','sum'],\n",
    "        'category_2_5.0': ['mean','sum'],\n",
    "        'category_3_A': ['mean','sum'],\n",
    "        'category_3_B': ['mean','sum'],\n",
    "        'category_3_C': ['mean','sum'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'state_id': ['nunique'],\n",
    "        'city_id': ['nunique'],\n",
    "        'subsector_id': ['nunique'],\n",
    "        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "        'purchase_month': ['mean', 'max', 'min', 'std','nunique'],\n",
    "        'purchase_date': [np.ptp, 'min', 'max','nunique'],\n",
    "        'month_lag': ['min', 'max','mean','sum',np.ptp],\n",
    "        'month_diff1': ['mean','max'],\n",
    "        'purchase_year':['nunique'],\n",
    "        'weekend': ['sum', 'mean']\n",
    "\n",
    "        }\n",
    "    for col in ['hour','weekofyear','dayofweek']:\n",
    "        agg_dict[col] = ['nunique']\n",
    "\n",
    "            \n",
    "    agg_tran_data = tran_data.groupby(['card_id']).agg(agg_dict)\n",
    "    agg_tran_data.columns = ['_'.join(col).strip() for col in agg_tran_data.columns.values]\n",
    "    agg_tran_data.reset_index(inplace=True)\n",
    "    \n",
    "    transactions_count = (tran_data.groupby('card_id').size().reset_index(name='transactions_count'))\n",
    "    agg_tran_data = pd.merge(transactions_count, agg_tran_data, on='card_id', how='left')\n",
    "    agg_tran_data.columns = [prefix + c if c != 'card_id' else c for c in agg_tran_data.columns]\n",
    "\n",
    "    # Step_2  Merging\n",
    "    df_train = pd.merge(df_train, agg_tran_data, on='card_id', how='left')\n",
    "    df_test = pd.merge(df_test, agg_tran_data, on='card_id', how='left')\n",
    "    return (df_train, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Features for 'train.csv' & 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', parse_dates=[\"first_active_month\"])\n",
    "df_test = pd.read_csv('test.csv', parse_dates=[\"first_active_month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'month' & 'year'  from 'first active month' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"month\"] = df_train[\"first_active_month\"].dt.month\n",
    "df_test[\"month\"] = df_test[\"first_active_month\"].dt.month\n",
    "df_train[\"year\"] = df_train[\"first_active_month\"].dt.year\n",
    "df_test[\"year\"] = df_test[\"first_active_month\"].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the elapsed time for each card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['elapsed_time'] = (datetime.date(2018, 2, 1) - df_train['first_active_month'].dt.date).dt.days\n",
    "df_test['elapsed_time'] = (datetime.date(2018, 2, 1) - df_test['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train, columns=['feature_1', 'feature_2'])\n",
    "df_test = pd.get_dummies(df_test, columns=['feature_1', 'feature_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Feature Engineering for transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 4.22 s, total: 1min 16s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_hist_trans = pd.read_csv('historical_transactions.csv',parse_dates=['purchase_date'])\n",
    "df_new_trans = pd.read_csv('new_merchant_transactions.csv',parse_dates=['purchase_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_trans['purchase_amount'] = np.round(df_new_trans.purchase_amount / 0.00150265118 + 497.06,2)\n",
    "df_hist_trans['purchase_amount'] = np.round(df_hist_trans.purchase_amount / 0.00150265118 + 497.06,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping 'authorized_flag', 'category_1' to 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.34 s, sys: 652 ms, total: 6 s\n",
      "Wall time: 5.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_hist_trans = map_yz_to_01(df_hist_trans)\n",
    "df_new_trans = map_yz_to_01(df_new_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.78 s, sys: 2.26 s, total: 8.03 s\n",
      "Wall time: 8.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_hist_trans = pd.get_dummies(df_hist_trans, columns=['category_2', 'category_3'])\n",
    "df_new_trans = pd.get_dummies(df_new_trans, columns=['category_2', 'category_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Features: weekofyear/dayofweek/weekend/hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.95 s, sys: 0 ns, total: 6.95 s\n",
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for df in [df_hist_trans,df_new_trans]:\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.dayofweek >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Features: purchase_year & purchase_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.03 s, sys: 4 ms, total: 3.04 s\n",
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_hist_trans['purchase_month'] = df_hist_trans['purchase_date'].dt.month\n",
    "df_new_trans['purchase_month'] = df_new_trans['purchase_date'].dt.month\n",
    "\n",
    "df_hist_trans['purchase_year'] = df_hist_trans['purchase_date'].dt.year\n",
    "df_new_trans['purchase_year'] = df_new_trans['purchase_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transfer month to continuous variable\n",
    "(df_hist_trans.loc[df_hist_trans['purchase_year']==2018,'purchase_month']) += 12\n",
    "(df_new_trans.loc[df_new_trans['purchase_year']==2018,'purchase_month']) += 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Features: month_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 1s, sys: 904 ms, total: 2min 2s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_hist_trans['month_diff1'] = ((datetime.date(2018, 2, 1) - df_hist_trans['purchase_date'].dt.date).dt.days)//30\n",
    "df_hist_trans['month_diff1'] += df_hist_trans['month_lag']\n",
    "df_new_trans['month_diff1'] = ((datetime.date(2018, 2, 1) - df_new_trans['purchase_date'].dt.date).dt.days)//30\n",
    "df_new_trans['month_diff1'] += df_new_trans['month_lag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating out unauthorized transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans_authorized = df_hist_trans[df_hist_trans['authorized_flag'] == 1]\n",
    "df_hist_trans_unauthorized = df_hist_trans[df_hist_trans['authorized_flag'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "for df in [df_hist_trans_authorized,df_hist_trans_unauthorized,df_new_trans]:\n",
    "    df.loc[df['installments']==-1,'installments'] = 0\n",
    "    df.loc[df['installments']==999,'installments'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_hist_trans\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Extrating Features from Transactions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 48s, sys: 3.53 s, total: 8min 51s\n",
      "Wall time: 8min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train, df_test = merging_transactions(df_hist_trans_authorized, df_train, df_test,'authorized_')\n",
    "df_train, df_test = merging_transactions(df_hist_trans_unauthorized, df_train, df_test,'unauthorized_')\n",
    "df_train, df_test = merging_transactions(df_new_trans, df_train, df_test,'new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917, 185)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123623, 183)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    df['hist_total_count'] = df.authorized_transactions_count + df.unauthorized_transactions_count\n",
    "    df.loc[df['hist_total_count'].isna(),'hist_total_count'] = df.loc[df['hist_total_count'].isna(),'authorized_transactions_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target']<-30,'outliers'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_hist_trans_authorized ,df_hist_trans_unauthorized ,df_new_trans\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.8 s, sys: 424 ms, total: 59.2 s\n",
      "Wall time: 59.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train.to_csv('train_new_clean.csv',index=False)\n",
    "df_test.to_csv('test_new_clean.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Distribution of Train/Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks_2sample test\n",
    "from scipy.stats import ks_2samp\n",
    "list_p_value =[]\n",
    "\n",
    "for i in df_test.columns[3:]:\n",
    "    list_p_value.append(ks_2samp(df_test[i] , df_train[i])[1])\n",
    "\n",
    "Se = pd.Series(list_p_value, index = df_test.columns[3:]).sort_values() \n",
    "list_discarded = list(Se[Se < .1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_purchase_amount_mean',\n",
       " 'authorized_purchase_amount_sum',\n",
       " 'unauthorized_category_2_1.0_mean',\n",
       " 'authorized_category_2_3.0_sum']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_purchase_amount_mean                     0.043279\n",
       "authorized_purchase_amount_sum               0.054246\n",
       "unauthorized_category_2_1.0_mean             0.070209\n",
       "authorized_category_2_3.0_sum                0.089007\n",
       "authorized_category_2_3.0_mean               0.100639\n",
       "new_month_lag_mean                           0.112871\n",
       "new_purchase_amount_sum                      0.121436\n",
       "authorized_purchase_amount_min               0.122635\n",
       "new_category_2_1.0_mean                      0.135876\n",
       "unauthorized_category_2_1.0_sum              0.139783\n",
       "authorized_category_2_1.0_mean               0.163847\n",
       "authorized_purchase_amount_max               0.171231\n",
       "new_category_2_3.0_mean                      0.177672\n",
       "authorized_purchase_amount_std               0.181861\n",
       "new_category_2_3.0_sum                       0.196148\n",
       "new_category_3_C_mean                        0.200871\n",
       "new_purchase_month_mean                      0.205667\n",
       "new_purchase_amount_max                      0.210023\n",
       "authorized_purchase_month_nunique            0.228300\n",
       "authorized_purchase_amount_mean              0.251899\n",
       "unauthorized_weekofyear_nunique              0.255859\n",
       "unauthorized_category_2_3.0_mean             0.258726\n",
       "unauthorized_purchase_amount_max             0.261085\n",
       "unauthorized_month_diff1_mean                0.270695\n",
       "authorized_month_diff1_mean                  0.282323\n",
       "authorized_purchase_date_ptp                 0.302355\n",
       "new_category_1_mean                          0.319883\n",
       "new_weekend_mean                             0.347772\n",
       "unauthorized_category_2_3.0_sum              0.350124\n",
       "new_city_id_nunique                          0.361218\n",
       "                                               ...   \n",
       "authorized_category_2_4.0_mean               0.998317\n",
       "unauthorized_state_id_nunique                0.999620\n",
       "feature_1_4                                  0.999646\n",
       "unauthorized_subsector_id_nunique            0.999673\n",
       "unauthorized_category_2_2.0_sum              0.999875\n",
       "unauthorized_category_2_4.0_sum              0.999877\n",
       "authorized_category_3_C_mean                 0.999912\n",
       "authorized_dayofweek_nunique                 0.999961\n",
       "authorized_category_2_2.0_mean               0.999965\n",
       "unauthorized_month_lag_max                   0.999971\n",
       "authorized_category_3_A_mean                 0.999982\n",
       "feature_2_1                                  0.999983\n",
       "authorized_category_2_4.0_sum                0.999985\n",
       "unauthorized_category_2_5.0_mean             0.999991\n",
       "unauthorized_category_2_4.0_mean             0.999994\n",
       "authorized_category_3_A_sum                  0.999996\n",
       "authorized_installments_max                  0.999996\n",
       "unauthorized_merchant_id_nunique             0.999998\n",
       "unauthorized_merchant_category_id_nunique    1.000000\n",
       "unauthorized_category_3_C_sum                1.000000\n",
       "unauthorized_category_2_2.0_mean             1.000000\n",
       "unauthorized_installments_min                1.000000\n",
       "authorized_category_2_2.0_sum                1.000000\n",
       "authorized_category_1_sum                    1.000000\n",
       "unauthorized_category_2_5.0_sum              1.000000\n",
       "year                                         1.000000\n",
       "authorized_installments_min                  1.000000\n",
       "feature_1_2                                  1.000000\n",
       "unauthorized_purchase_year_nunique           1.000000\n",
       "feature_1_1                                  1.000000\n",
       "Length: 180, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Se"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "185px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
